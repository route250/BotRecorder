---
marp: true
math: katex
---
<!-- paginate: true -->
<!-- header: チャットbotに エコーキャンセルを！！ -->
<!-- footer: 2024-09-29 Osaka GPT ＃1.9 -->

# チャットボットに エコーキャンセルを！！

---

## AIの話に割り込めない

AIと話していると、どうしても「今のは違うよ！」って途中で言いたくなる瞬間、ありますよね。でも、AIは一度話し始めると延々と話し続け、途中で割り込むことができません。これって人間との会話とは大きく異なる点です。人間同士の会話なら、話を遮ったり、途中でツッコミを入れるのは普通ですからね。

![bg right 90%](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/1cd13bb2-f738-3176-7417-088a128adf1c.png)

---
## どうすりゃいいの？

じゃあ、どうやってこの「割り込みたい」問題を解消するか？以下のような方法が思い付きます。

- マルチモーダルなLLM
- 話者認識できる Speech to Text
- 音響エコーキャンセリング

---

### 1. マルチモーダルなLLM

音声そのものを学習させたLLMならば、自然な会話が可能。

- OpenAI Advanced Voice  
- Google Gemini Live
- kyutai moshi  
- その他、多数

しかし、サービスを利用するしかなさそう。自分でできることはなし。

---

### 2. 話者認識できる Speech to Text

話者認識機能を持ったSTTを使えば、誰が話しているのかを識別できる。
従って、AIの話した部分を除いて、認識できる。(はず)

- **Google Speech** や各社のサービス
- Pythonのライブラリにも使えそうなのがあります

しかし、リアルタイムな会話に使うには難しそう。
1) ある程度の長さ(会議の書き起こしなど)の音声データが必要(みたい?)
2) 計算が重い(GPUが必要なケースが多い)

---

### 3. 音響エコーキャンセリング

録音した音声データから、不要な部分を除去する処理

DSPなどのハードウエアで処理していることが多い。
- スピーカーフォン、ヘッドセット

しかし、ドライバやアプリ側で処理している例もある。
- オンライン会議システム(zoom,teams,discode)

それっぽいpythonライブラリも多数。しかし、OSはハードに依存してたりして、いまいち使いにくい。
- pymic, sppexdsp, その他

**自分でもpythonでも実装できるかも?？**

https://smile-jsp.hateblo.jp/entry/2020/03/08/220631

---
# エコーキャンセルを実装してみた

## アルゴリズムどれにする？

簡単なヤツ、難しそうなヤツ、特許モノ 各種様々。

リアルタイム性と自分の力量を考えて

- LMS(Least Mean Squares)適用フィルタ
 ==> 計算が軽くて簡単そう。最も基本的

実際にデバッグしている途中で、NLMSになりました。

---

### NLMSアルゴリズム

$$ 予測式 : y(n) = \mathbf{w}^T(n) \mathbf{x}(n) $$

$$ 誤差 : e(n) = d(n) - y(n)$$

$$ フィルタ係数更新 : \\
\mathbf{w}(n+1) = \mathbf{w}(n) + \mu \frac{e(n) \mathbf{x}(n)}{\mathbf{x}^T(n) \mathbf{x}(n) + \delta}
$$

    x(n) : マイク入力　        w(n) : フィルタ係数　     y(n) : 予測信号
    d(n) : スピーカー出力　     e(n) : 誤差
    μ: 学習率（ステップサイズ）　 δ: ゼロ割保護

数式は得意でないので詳しくは、各自調べてください。

---
|||||||||||
|---:|---|---|---|---|---|---|---|---|---|
|マイク x(n)|・・・|x4|x5|x6|x7|x8|x9|x10|・・・|
|係数 w(n)||||w1|w2|w3|w4|w5|
|予測値 y(n)||||||||y10 = x5 * w1+・・・+x9 * w5|
|スピーカ d(n)||||||||d10|
|誤差 e(n)||||||||e10=d10-y10|

---

## 参照信号(スピーカ音声)をどうする？

適用フィルタの処理には、マイク録音音声と、スピーカ音声が必要
通常は、二つのマイクで同時録音
しかし、一般的なPCにはマイクは一つだけ

そこで、
再生する音声データそのものを使う
つまり、音声合成した音声データ配列をそのまま使います。

---

# 音の先頭はどこですか？？？

適用フィルタを使うためには、音の先頭を一致させる必要があります。

マイクの音とスピーカーの音の先頭が狂っていると、関係ない箇所のデータでフィルタの重みを計算することになるので、めちゃくちゃな音ができあがります(たぶん)。

つまり、pythonで再生した音は、いつマイクで録音されるのか？？これを可能なかぎり正確に把握する必要があります。

---

# 計測してみた

目標音（440Hzと880Hzの合成音、0.2秒）を再生し、マイクでの録音される時間を測定

- **Linux**: 0.8〜1.2秒
- **MacBook**: 0.5〜0.6秒

![bg right height:250](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/3255875a-c5b8-9088-fc19-a88ce88522d7.png)

---

## どうしてこんなに時間がかかるの？

pythonで再生の関数をコールした瞬間にスピーカーから音がでれば良いのですが、そんなことはありません。実際には、pythonのライブラリ、OSのライブラリ・ドライバ、ハードウエアを経由するので時間がかかっていると思います。

![bg right h:70%](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/fd5e26f5-a0fa-c46d-8f40-7fccc79c0f05.png)

---

## LMS適用フィルタの係数の幅はどれくらい必要？

フィルタの係数の幅は、短かければ計算が軽くなりますが効果が低くなりそうです。反対に長くすれば計算が増えてリアルタイムな処理ができなくなります。


$$ 予測式 : y(n) = \mathbf{w}^T(n) \mathbf{x}(n) $$


どれくらいの幅が必要か見積もってみました。

---

### スピーカーから直接マイクに音が到達する場合

音の速度 : 約340 m/sec
スピーカーからマイクの距離 : 30cm
到達時間: 約0.8ms

サンプリングレート16kHzで
約14サンプルの遅延

### 音の位置合わせ
それなりに高精度でないといけない

![bg right height:300](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/63b9d2c9-c5b2-d273-5ae9-4a41ddfc640d.png)

---

### 反射してマイクに音が入る場合

往復距離 : 8m
到達時間 : 約23ms

サンプリングレート16kHzで
約376サンプルの遅延

### フィルタ係数の幅
最低でも300〜400サンプル

![bg right height:300](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/63b9d2c9-c5b2-d273-5ae9-4a41ddfc640d.png)

---

# コーディング and テスト

色々と苦労した挙句、いまいちな結果に終わる

---

# エコーキャンセルの結果

NLMSフィルタでエコーキャンセルしたデータのプロット

![height:300](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3734397/834b59a4-f1ef-4db2-a409-fe81cfd7dbfe.png)

 - エコーは1/3〜1/4程度軽減
 - しかし、音声認識モデル（WhisperやGoogle Speech）は非常に強力で、この程度のエコー軽減では音声認識されてしまいます。

---

# 最後に

今回の実験で、LMSフィルタによるエコーキャンセルがある程度効果を発揮することが確認できましたが、完璧な結果にはまだ至りませんでした。特に、WhisperやGoogle Speechのような高度な音声認識システムに対しては、さらなる調整やアルゴリズムの改良が必要です。

しかし、フィルタ係数やアルゴリズムをさらに最適化することで、リアルタイムでの音声会話AIの実現に近づける可能性は十分にあります。

---
# 「私」の生まれる場所

https://tenro-in.com/mediagp/readinglife-science/66921/
